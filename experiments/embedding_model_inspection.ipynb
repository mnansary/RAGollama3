{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the created database\n",
    "VECTOR_DB_PATH=\"/home/vpa/RAGollama3/data/20241006/database\"\n",
    "\n",
    "# The path of the csv file that will be used to create a vectorstore. \n",
    "# # The csv will have only one column and each row entry will be treated as a separate document.  \n",
    "DATA_SOURCE=\"/home/vpa/RAGollama3/data/20241006/source.csv\" \n",
    "\n",
    "# The model to use in sentence-transformers for creation embedding\n",
    "EMBEDDING_MODEL=\"l3cube-pune/bengali-sentence-similarity-sbert\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Define the CustomEmbeddings class\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self, model_name: str, device: str = None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents (texts) into embeddings.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query into an embedding.\"\"\"\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------\n",
    "# imports\n",
    "#---------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "#--------------------------------------\n",
    "# global\n",
    "#--------------------------------------\n",
    "embedding_model=CustomEmbeddings(EMBEDDING_MODEL)\n",
    "\n",
    "#--------------------------------------\n",
    "# helper functions\n",
    "#--------------------------------------\n",
    "def create_vector_store(df: pd.DataFrame, \n",
    "                        vector_db_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a vector store from a dataframe with a single column, where each entry in the dataframe\n",
    "    is treated as a separate document.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a single column with text data.\n",
    "        vector_db_path (str): Path to the directory where the vector store will be persisted.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available and set the device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create data directory structure\n",
    "    os.makedirs(vector_db_path, exist_ok=True)\n",
    "\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Initialize the vector store with GPU-based embeddings\n",
    "    vectorstore = Chroma(persist_directory=vector_db_path, embedding_function=embedding_model)\n",
    "\n",
    "    # Process each document in the dataframe\n",
    "    for data in tqdm(df.iloc[:, 0]):\n",
    "        documents = [Document(page_content=data)]\n",
    "        all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Add the split documents to the vector store\n",
    "        vectorstore.add_documents(all_splits)\n",
    "\n",
    "    # Persist the vector store\n",
    "    vectorstore.persist()\n",
    "    print(\"Vector store created and persisted at:\", vector_db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(DATA_SOURCE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_store(df,VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Validity of vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_vector_store(vector_db_path: str, query_text: str, embedding_model, top_k: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Query the vector store to find and display the most similar documents to the given query text.\n",
    "\n",
    "    Args:\n",
    "        vector_db_path (str): Path to the directory where the vector store is persisted.\n",
    "        query_text (str): The text query for which to retrieve similar documents.\n",
    "        embedding_model: The embedding function used to encode the query.\n",
    "        top_k (int): The number of top similar documents to retrieve.\n",
    "    \"\"\"\n",
    "    # Load the vector store\n",
    "    vectorstore = Chroma(persist_directory=vector_db_path, embedding_function=embedding_model)\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Perform the query\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(query_text, k=top_k)\n",
    "    \n",
    "    # Print the results\n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"Similarity score:\", score)\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query_vector_store(vector_db_path=VECTOR_DB_PATH, query_text='ডেসকোর মোবাইল নাম্বার কত?',embedding_model=embedding_model, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
